---
layout: post
title: a post with TikZJax
date: 2023-12-12 22:25:00
description: this is what included TikZ code could look like
tags: formatting diagrams
categories: sample-posts
tikzjax: true
---

## INTRODUCTION

This exercise aims on developing a classification model utilizing logistic regression. The primary objective is to construct a logistic model to determine whether a given observation indicates that the 8-path liquid ultrasonic flow meter is healthy or if it has been influenced by installation effects.

The data processing phase consists of several key steps, including reading data from a text file, preprocessing it, and splitting the dataset for training and testing purposes. These steps are essential for preparing the data for effective classification.\\
The model will be evaluated on both training and test sets, and we will monitor the convergence behavior throughout the optimization process by tracking the number of iterations. Various predictors, such as flatness, ratio, symmetry, crossflow, and flow velocity, will be utilized to assess whether a given experimental observation of the ultrasonic flow meter indicates a healthy state or reveals any installation effects. Through this approach, we aim to build a model capable of accurately classifying the state of the flow meter based on the available data.

## ABOUT THE DATASET

The dataset consists of sample measurements of the ultrasonic flow meters \cite{ultrasonic_flowmeter_diagnostics_433}. It is called 'Meter A' and it contains 87 instances of diagnostic parameters for an 8-path liquid ultrasonic flow meter (USM). It has 36 continuous independent attributes and one discrete dependent variable with classes of two health states: Healthy with numerical value 1 and installation effect with 2.

## DATA PREPROCESSING##

Feature matrix (X) was separated from the target (y) and the class attribute was converted to binary classification 1 and 0 where 1 represents the healthy condition and 0 represents the installation effect. The predictors are so sparse that they need to be normalized as presented in Figure \ref{fig1}. To center and scale them, we have adopted the 'Z-score' technique calculated by the formula \ref{eq1}
\begin{equation}\label{eq1}
z = \frac{(x - \mu)}{\sigma},
\end{equation}
where $x$ is the raw score, $\mu$ is the population mean, and $\sigma$ is the population standard deviation.

After the Z-score transformation, Figure \ref{fig1} is transformed into Figure \ref{fig2}. Additionally, we examined the dataset for missing values and confirmed that there were none present. Furthermore, we assessed the dataset for categorical variables that required removal or encoding and found no such variables.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{box1.jpg}
\caption{Box plot of data before standardization}
\label{fig1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{box2.jpg}
\caption{Box plot of data after standardization}
\label{fig2}
\end{figure}

\subsection{TRAIN TEST SPLIT}

Given the limited size of our dataset, we employed a two-way splitting approach to divide it into training and test sets. However, due to the sequential alignment of the data where healthy observations were aligned first and followed by those installation effects observations, we decided to shuffle the dataset before splitting to ensure a more representative distribution in both sets.

To gain a clearer understanding of our model, we experimented with various training set sizes. We began with a 60:40 ratio, followed by 65:35, 70:30, 75:25, and finally, an 80:20 ratio. This approach allowed us to assess how the size of the training set affected the model's predictive accuracy and generalization capabilities.

\section{MODEL IMPLEMENTATION AND METHODOLOGY }
\subsection{LOGISTIC REGRESSION}

Logistic regression is used for classification tasks. It attempts to model the relationship between two or more explanatory variables and a response variable to predict the probability that an instance belongs to a given class or not.

It uses a sigmoid function to predict the probabilities as the function output ranges from 0 to 1.
\begin{equation}\label{eqn2}
\sigma(x) = \frac{1}{1 + e^{-x}},
\end{equation}
where $x$ is the input value and $e$ is the base of natural logarithm.

\subsection{THE COST FUNCTION AND OPTIMIZATION}
The negative log-likelihood function is employed as the cost function for logistic regression:

\begin{equation}
\text{Cost}(\beta) = -\sum\_{i=1}^{n} \left[ y_i \log(\sigma(X_i \beta)) + (1 - y_i) \log(1 - \sigma(X_i \beta)) \right]
\end{equation}
where \( n \) is the number of observations, and \( y_i \) is the actual class label for the \( i \)-th observation.

Note that we initialized the weight as zero and embedded the bis term within the data matrix X.

For parameter optimization, we employed the \texttt{`fminsearch'} algorithm in MATLAB, utilizing the negative log-likelihood function as our cost function. A custom output function was created to track the loss history and the number of iterations during the optimization process. \\
Importantly, we restricted ourselves from explicitly setting stopping criteria to avoid premature convergence, allowing the algorithm to determine the optimal parameters based on the data.

\subsection{EVALUATION METRICS}

The evaluation of our logistic regression model was done using several key metrics to assess its performance. \\After training, predictions were generated on both the training and test datasets. The primary evaluation metric used was accuracy, which measures the proportion of correctly classified observations out of the total. Additionally, a confusion matrix was generated to provide a detailed breakdown of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). By visualizing the confusion matrix, we could better understand the model's strengths and weaknesses in terms of misclassifications. Based on the confusion matrices, Recall and Precision were calculated as well.

\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\newpage
\section{RESULTS AND DISCUSSION}
\label{sec4}

\subsection{OPTIMIZATION}
\begin{figure}[H]
\centering

    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fminsearch60.jpg}
        \caption{Convergence track with 60:40 split ratio}
        \label{fig:box1}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \textit{This figure illustrates the convergence of the loss function for the 60:40 training-to-test split. The negative log-likelihood demonstrates a consistent decline as the iterations progress. This one shows the most significant initial drop among all the splits, with the cost function starting around 36. Furthermore, the optimization process finished after 6,601 iterations.}
    \end{minipage}

    \vspace{0.5cm} % space between figures

    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fminsearch75.jpg}
        \caption{Convergence track with 75:25 split ratio}
        \label{fig:box2}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \textit{This figure shows the convergence of the loss function for the 75:25 training-to-test split. The negative log-likelihood demonstrates a consistent decline as the iterations progress. However, it is not as steep as the previous split because the training data has increased. The optimization process finished after 6,613 iterations.}
    \end{minipage}

    \vspace{0.5cm} % space between figures

    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fminsearch80.jpg}
        \caption{Convergence track with 80:20 split ratio}
        \label{fig:box3}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \textit{Visualize this figure with the 80:20 split; there is no really big difference from the 75:25 because the dataset quantity is not that far. It required 6,558 iterations for the optimization to complete. We observe that as we increase the size of the training dataset, more optimization iterations are required. Therefore, this tells us that if we have a lot of data, it is better to specify the stopping criterion.}
    \end{minipage}

\end{figure}

\subsection{MODEL PERFORMANCE: CLASSIFICATION ACCURACY}

As already stated before, we applied logistic regression from which we got several values that can be used in the analysis and prediction: model coefficients (beta values) and accuracy percentage. With different split, the training and testing accuracy percentages are presented in the table below:

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5} % Increase the row height for better readability
\setlength{\tabcolsep}{12pt} % Add horizontal padding between columns
\begin{tabular}{lcc}
\toprule
\textbf{Split Ratio} & \textbf{Training Accuracy (\%)} & \textbf{Testing Accuracy (\%)} \\
\midrule
\textbf{60:40} & 80.77 & 82.86 \\
\textbf{75:25} & 83.08 & 100.00 \\
\textbf{80:20} & 90.00 & 76.47 \\
\bottomrule
\end{tabular}
\caption{Classification accuracy for different train-test split ratios}
\label{tab:classification_accuracy}
\end{table}

The beta values represent the contribution of each feature to the model's prediction. While each split presents a different scenario, the 25th (gain at the fifth end of each of the eight paths) variable consistently exhibits the highest impact.

In the 80:20 split, the 25th variable is the most significant contributor, with a beta value reaching 21.5878. This is notably higher compared to the other variables, which range between -10 and 10.

For the 60:40 split, the 25th variable again shows a strong influence, though negative in this case, with a value of -6.0854, while the other variables remain within the range of -4 to +4.5.

Finally, in the 75:25 split, most features fall within a narrow range of -3 to 3, but the 25th variable still stands out with a comparatively higher magnitude, underscoring its continued significance across different splits.
\subsection{MODEL PERFORMANCE: CONFUSION MATRICES}
\begin{multicols}{3}
\begin{minipage}{\linewidth}
\includegraphics[width=\linewidth]{conf60.jpg}
\caption{\small{Confusion matrix for 60:40 split}}
\label{fig:conf60}
\vspace{0.5cm} % Space between figure and text
\\
\textit{Precision }$= \frac{18}{18+5} = 0.7826$.\\
\textit{Recall }$= \frac{18}{18+1} = 0.9474$.\\
\textit{This value of precision shows that out of all predicted observations as healthy, 78.26\% were actually healthy. The Recall value tells us that out of all healthy observations, 94.74\% were predicted as healthy.}
\end{minipage}

    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{conf75.jpg}
        \caption{\small{Confusion matrix for 75:25 split}}        \label{fig:conf75}
        \vspace{0.5cm} % Space between figure and text
        \\
        \textit{There is no use to calculate recall or precision because it is clear that it 100\% Because there was no misclassifications here.}
    \end{minipage}

    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{conf80.jpg}
        \caption{\small{Confusion matrix for 80:20 split}}
        \label{fig:conf80}
        \vspace{0.5cm} % Space between figure and text
        \\
        \textit{Precision }$= \frac{6}{6+3} = 0.6667$.\\
        \textit{Recall }$= \frac{6}{7} = 0.8571$.\\
        \textit{This value of precision shows that out of all predicted observations as healthy, 66.67\% were actually healthy. The Recall value tells us that out of all healthy observations, 85.71\% were predicted as healthy.}
    \end{minipage}

\end{multicols}
