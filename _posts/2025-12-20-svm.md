<!-- ---
layout: post
title: Nonlinear Support Vector Machine (SVM) with Polynomial Kernel
date: 2025-12-20
description: Nonlinear support vector machine from scratch
tags: formatting images
categories: sample-posts
thumbnail: assets/img/svm_overall.png
---

## Support Vector Machines (SVM)

Support Vector Machines (SVMs) are powerful supervised learning models used for both classification and regression. Unlike probabilistic models, SVMs focus on finding a decision boundary that maximizes the **margin** between classes, leading to strong generalization properties.

## Maximum Margin Principle

At the core of SVM lies the idea of selecting the hyperplane that maximizes the distance between itself and the closest data points from each class, known as **support vectors**.

Mathematically, this corresponds to solving the optimization problem:

$$
\min_{\mathbf{w}, b} \; \frac{1}{2}\|\mathbf{w}\|^2
\quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1
$$

This formulation encourages large margins while correctly classifying training samples.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/svm_vid.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>

<div class="caption">
  Illustration of the maximum margin principle. The optimal hyperplane is determined solely by the support vectors, while other points have no influence on the decision boundary.
</div>

---

## Soft Margin and Regularization

Real-world data are rarely perfectly separable. To handle overlap and noise, SVM introduces **slack variables** and a regularization parameter C, allowing controlled violations of the margin constraint.

- Large C: prioritizes correct classification (low bias, high variance)
- Small C: prioritizes wider margins (higher bias, better generalization)

This trade-off is crucial for robust performance.

---

## The Kernel Trick

One of the most powerful aspects of SVM is the **kernel trick**, which enables nonlinear decision boundaries without explicitly mapping data to high-dimensional spaces.

Common kernels include:

- Linear
- Polynomial
- Radial Basis Function (RBF)
- Sigmoid

---

## Why SVMs Work Well

SVMs are particularly effective when:

- the dataset is small or medium-sized
- the number of features is high
- clear margins exist between classes

However, they can be computationally expensive for very large datasets and require careful kernel and hyperparameter selection.

---

## Remarks

Support Vector Machines provide a geometrically intuitive and theoretically grounded approach to supervised learning. By focusing on margin maximization and leveraging kernel methods, SVMs remain a strong baseline and a valuable tool in modern machine learning pipelines.

---

## Project Implementation

Support Vector Machines (SVMs) are powerful supervised learning models for classification tasks. While linear SVMs work on linearly separable data, this implementation extends SVM to nonlinear classification using a polynomial kernel. We solve the optimization problem using cvxopt quadratic programming solver.

We implement a Nonlinear Support Vector Machine (SVM) from scratch in Python using Quadratic Programming (QP) with the help of the cvxopt optimization library.
The model uses a polynomial kernel to separate non-linearly separable data and is tested on a small dataset (t030.csv) [see github page].

The implementation includes:

- Training SVM with a polynomial kernel
- Identifying support vectors
- Plotting decision boundaries

```python
import numpy as np
import matplotlib.pyplot as plt
from cvxopt import matrix, solvers

def polynomial_kernel(x1, x2, degree=2):
    """Compute the polynomial kernel between two sets of samples."""
    return (1 + np.dot(x1, x2.T)) ** degree

def svm_nonlinear(traindata, trainclass, C):
    """Train a nonlinear SVM with a polynomial kernel."""

    trainclass = np.where(trainclass == 2, -1, 1)
    m = traindata.shape[0]

    # Compute the kernel matrix
    K = polynomial_kernel(traindata, traindata)

    # matrices for the QP problem
    P = matrix(np.outer(trainclass, trainclass) * K)
    q = matrix(-np.ones(m))
    G = matrix(-np.eye(m))
    h = matrix(np.zeros(m))
    A = matrix(trainclass, (1, m), 'd')
    b = matrix(0.0)

    # Solve the QP problem
    sol = solvers.qp(P, q, G, h, A, b)

    # Get the Lagrange multipliers
    alphas = np.ravel(sol['x'])

    # Identify support vectors
    sv_idx = alphas > 1e-5
    alphas = alphas[sv_idx]
    svs = traindata[sv_idx]
    sv_classes = trainclass[sv_idx]

    # Compute the bias term
    w0 = np.mean([sv_classes[i] - np.sum(alphas * sv_classes * polynomial_kernel(svs[i], svs)) for i in range(len(svs))])

    return svs, sv_classes, alphas, w0
```

# nice plotting function

````python
def plot_decision_boundary(traindata, trainclass, svs, sv_classes, alphas, w0):
    """Plot the decision boundary and the support vectors."""
    plt.figure(figsize=(10, 6))

    # Plot the training data
    plt.scatter(traindata[trainclass == 1][:, 0], traindata[trainclass == 1][:, 1], color='blue', label='Class 1', s=100)
    plt.scatter(traindata[trainclass == 2][:, 0], traindata[trainclass == 2][:, 1], color='red', label='Class 2', s=100)

    # Plot support vectors
    plt.scatter(svs[:, 0], svs[:, 1], facecolors='none', edgecolors='k', s=300, label='Support Vectors')

    # Create a grid to plot the decision boundary
    xlim = plt.xlim()
    ylim = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))

    # Evaluate decision function on the grid
    grid = np.c_[xx.ravel(), yy.ravel()]
    K_grid = polynomial_kernel(grid, svs)
    decision_scores = np.dot(K_grid, alphas * sv_classes) + w0
    decision_scores = decision_scores.reshape(xx.shape)

    # Plot the decision boundary and margins
    plt.contour(xx, yy, decision_scores, levels=[0], linewidths=2, colors='k')
    plt.title('Nonlinear SVM with Polynomial Kernel')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.grid()
    plt.show()

    ```
```` -->

---

layout: post
title: Nonlinear Support Vector Machine (SVM) with Polynomial Kernel
date: 2025-12-20
description: Understanding and implementing nonlinear support vector machines from scratch
tags: machine-learning svm classification kernel-methods
categories: machine-learning
thumbnail: assets/img/svm_overall.png

---

## Introduction to Support Vector Machines

Support Vector Machines (SVMs) are powerful supervised learning algorithms designed for classification and regression tasks. What makes SVMs special is their unique approach to finding decision boundaries. Instead of trying to fit a probabilistic model to the data, SVMs search for the **optimal separating hyperplane** that creates the largest possible margin between different classes.

Think of it this way: if you're drawing a line to separate two groups of points, you want that line to be as far away as possible from both groups. This "safety margin" helps the model generalize better to new, unseen data.

---

## The Core Idea: Maximum Margin Principle

The fundamental principle behind SVM is **margin maximization**. The margin is the perpendicular distance from the decision boundary (hyperplane) to the nearest data points from each class. These nearest points are called **support vectors** because they literally "support" or define the decision boundary.

### Why Maximize the Margin?

A larger margin provides:

- **Better generalization**: The model is less sensitive to small variations in the data
- **Robustness**: New data points are less likely to be misclassified
- **Geometric intuition**: The decision is based on clear separation, not just probability

### The Mathematical Formulation

For a dataset with features **$x$** and binary labels **$y$** (where $y = +1$ or $y = -1$), we want to find a hyperplane defined by weights **$w$** and bias **$b$** such that:

$$
\min_{\mathbf{w}, b} \; \frac{1}{2}\|\mathbf{w}\|^2
\quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1
$$

**Breaking this down:**

- The term $\frac{1}{2}|w|^2$ measures the inverse of the margin (smaller **w** means larger margin)
- The constraint $y_i(w^TX_i + b) \ge 1$ ensures all points are correctly classified with at least distance 1 from the boundary
- Only the support vectors (points closest to the boundary) affect the solution

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/svm_vid.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>

<div class="caption">
  Illustration of the maximum margin principle. The optimal hyperplane is determined solely by the support vectors (the points touching the margin boundaries), while other points have no influence on the decision boundary.
</div>

---

## Handling Real-World Data: Soft Margin SVM

In practice, data is rarely perfectly separable. Classes often overlap due to noise, outliers, or the inherent nature of the problem. This is where **soft margin SVM** comes in.

### Introducing Slack Variables

Soft margin SVM introduces slack variables $(\xi_i)$ that allow some points to violate the margin constraint. The optimization problem becomes:

$$
\min_{\mathbf{w}, b, \xi} \; \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{m}\xi_i
\quad \text{subject to} \quad y_i(\mathbf{w}^\top x_i + b) \ge 1 - \xi_i, \; \xi_i \ge 0
$$

### The Regularization Parameter C

The parameter **C** controls the trade-off between margin maximization and classification errors:

- **Large C** (e.g., $C = 1000$):
  - Penalizes misclassifications heavily
  - Results in narrower margins
  - Lower bias, higher variance
  - Risks overfitting

- **Small C** (e.g., $C = 0.1$):
  - Allows more margin violations
  - Results in wider margins
  - Higher bias, lower variance
  - Better generalization

**Practical tip**: Start with $C = 1.0$ and tune using cross-validation.

---

## The Kernel Trick: Unlocking Nonlinear Classification

This is where SVM becomes truly powerful. Many real-world problems cannot be solved with a straight line (or hyperplane). The data might be arranged in circles, spirals, or other complex patterns.

### What is the Kernel Trick?

The kernel trick is a mathematical technique that allows us to:

1. Implicitly map data to a higher-dimensional space
2. Find a linear separator in that high-dimensional space
3. **Never actually compute the high-dimensional coordinates**

Instead of explicitly transforming each data point $\phi(x)$, we use a kernel function $K(x_1, x_2)$ that computes the inner product in the transformed space directly.

### Common Kernel Functions

#### 1. Linear Kernel

$$
K(\mathbf{x}_1, \mathbf{x}_2) = \mathbf{x}_1^\top \mathbf{x}_2
$$

- Use case: Data is already linearly separable
- Fastest to compute

#### 2. Polynomial Kernel

$$
K(\mathbf{x}_1, \mathbf{x}_2) = (1 + \mathbf{x}_1^\top \mathbf{x}_2)^d
$$

- Parameter: degree **d** (typically 2-5)
- Use case: Data has polynomial decision boundaries
- Our implementation uses this kernel!

#### 3. Radial Basis Function (RBF/Gaussian) Kernel

$$
K(\mathbf{x}_1, \mathbf{x}_2) = \exp\left(-\gamma \|\mathbf{x}_1 - \mathbf{x}_2\|^2\right)
$$

- Parameter: $\gamma$ (controls influence radius)
- Use case: Most versatile, works for many nonlinear patterns
- Default choice when in doubt

#### 4. Sigmoid Kernel

$$
K(\mathbf{x}_1, \mathbf{x}_2) = \tanh(\alpha \mathbf{x}_1^\top \mathbf{x}_2 + c)
$$

- Use case: Similar to neural network activation
- Less commonly used

### How the Kernel Trick Works

Consider a simple example with polynomial kernel of degree 2:

- Original space: 2D points $(x_1, x_2)$
- Transformed space: 6D $(1, x_1, x_2, x_1^2, x_2^2, x_1x_2)$
- Direct computation: $O(d^2)$ operations
- Kernel computation: $O(d)$ operations

The kernel computes the same result as the inner product in the transformed space, but much more efficiently!

---

## When to Use SVMs

### SVMs Excel When:

1. **Small to medium-sized datasets** (hundreds to thousands of samples)
   - Computational cost grows with dataset size
   - Training time is $O(n^2)$ to $O(n^3)$

2. **High-dimensional feature spaces** (many features relative to samples)
   - SVMs are less prone to overfitting in high dimensions
   - Effective even when features >> samples

3. **Clear margin separation exists**
   - SVMs naturally find the best separating boundary
   - Robust to outliers (with proper C tuning)

4. **Interpretability matters**
   - Decision boundary is defined by support vectors
   - Can identify which samples are most important

### Consider Alternatives When:

- Dataset is very large (>100,000 samples) $\to$ Use logistic regression or neural networks
- Data is very noisy with no clear separation $\to$ Try ensemble methods
- You need probability estimates $\to$ Use logistic regression or calibrated classifiers
- Real-time prediction speed is critical $\to$ Simpler models may be faster

---

## Implementation from Scratch

Now let's implement a nonlinear SVM using Python! We'll use the **polynomial kernel** and solve the optimization problem using quadratic programming.

### Step 1: The Polynomial Kernel Function

```python
import numpy as np
import matplotlib.pyplot as plt
from cvxopt import matrix, solvers

def polynomial_kernel(x1, x2, degree=2):
    """
    Compute the polynomial kernel between two sets of samples.

    Parameters:
    -----------
    x1 : ndarray of shape (n_samples_1, n_features)
        First set of samples
    x2 : ndarray of shape (n_samples_2, n_features)
        Second set of samples
    degree : int, default=2
        Degree of the polynomial kernel

    Returns:
    --------
    K : ndarray of shape (n_samples_1, n_samples_2)
        Kernel matrix where K[i,j] = kernel(x1[i], x2[j])

    Mathematical formula: K(x1, x2) = (1 + x1^T * x2)^degree
    """
    return (1 + np.dot(x1, x2.T)) ** degree
```

**Why add the +1?** The constant term ensures that the kernel includes lower-degree polynomial terms as well. For example, with degree=2, we get terms involving x₁², x₂², x₁x₂, x₁, x₂, and the constant.

### Step 2: Training the Nonlinear SVM

```python
def svm_nonlinear(traindata, trainclass, C=1.0):
    """
    Train a nonlinear SVM with a polynomial kernel using Quadratic Programming.

    Parameters:
    -----------
    traindata : ndarray of shape (n_samples, n_features)
        Training feature vectors
    trainclass : ndarray of shape (n_samples,)
        Training labels (assumed to be 1 and 2, will be converted to +1 and -1)
    C : float, default=1.0
        Regularization parameter (soft margin parameter)

    Returns:
    --------
    svs : ndarray
        Support vectors (the samples that define the decision boundary)
    sv_classes : ndarray
        Labels of support vectors
    alphas : ndarray
        Lagrange multipliers for support vectors
    w0 : float
        Bias term (intercept of the decision boundary)
    """

    # Convert labels from {1, 2} to {+1, -1} for mathematical convenience
    trainclass = np.where(trainclass == 2, -1, 1)
    m = traindata.shape[0]  # Number of training samples

    # Step 1: Compute the Kernel Matrix
    # K[i,j] represents the kernel similarity between sample i and sample j
    K = polynomial_kernel(traindata, traindata)

    # Step 2: Set up the Quadratic Programming problem
    # We're solving: min (1/2)α^T P α + q^T α
    #           subject to: Gα ≤ h and Aα = b

    # P matrix: (y_i * y_j) * K(x_i, x_j)
    P = matrix(np.outer(trainclass, trainclass) * K)

    # q vector: all -1s (from the dual formulation)
    q = matrix(-np.ones(m))

    # G and h: inequality constraints (α_i ≥ 0)
    # We use -I and 0 to represent α_i ≥ 0
    G = matrix(-np.eye(m))
    h = matrix(np.zeros(m))

    # A and b: equality constraint (Σ α_i * y_i = 0)
    A = matrix(trainclass, (1, m), 'd')
    b = matrix(0.0)

    # Step 3: Solve the QP problem
    solvers.options['show_progress'] = False  # Suppress solver output
    sol = solvers.qp(P, q, G, h, A, b)

    # Step 4: Extract Lagrange multipliers
    alphas = np.ravel(sol['x'])

    # Step 5: Identify support vectors
    # Support vectors have α > 0 (we use a small threshold for numerical stability)
    sv_idx = alphas > 1e-5
    alphas = alphas[sv_idx]
    svs = traindata[sv_idx]
    sv_classes = trainclass[sv_idx]

    # Step 6: Compute the bias term w0
    # We use the KKT conditions: for support vectors on the margin,
    # y_i * (Σ α_j * y_j * K(x_i, x_j) + w0) = 1
    # Therefore: w0 = y_i - Σ α_j * y_j * K(x_i, x_j)
    # We average over all support vectors for numerical stability
    w0 = np.mean([
        sv_classes[i] - np.sum(alphas * sv_classes * polynomial_kernel(svs[i].reshape(1, -1), svs))
        for i in range(len(svs))
    ])

    return svs, sv_classes, alphas, w0
```

**Key concepts in the code:**

1. **Quadratic Programming**: SVM training is formulated as a QP problem in the dual form
2. **Support Vectors**: Only points with α > 0 actually contribute to the decision boundary
3. **Kernel Matrix**: All pairwise similarities are precomputed for efficient optimization
4. **Bias Computation**: Averaged over support vectors for better numerical stability

### Step 3: Making Predictions

```python
def predict_svm(X, svs, sv_classes, alphas, w0):
    """
    Make predictions using the trained SVM.

    Parameters:
    -----------
    X : ndarray of shape (n_samples, n_features)
        Samples to predict
    svs : ndarray
        Support vectors from training
    sv_classes : ndarray
        Labels of support vectors
    alphas : ndarray
        Lagrange multipliers
    w0 : float
        Bias term

    Returns:
    --------
    predictions : ndarray
        Predicted class labels (+1 or -1)
    decision_scores : ndarray
        Raw decision function values (distance from boundary)
    """
    # Compute kernel between test points and support vectors
    K = polynomial_kernel(X, svs)

    # Decision function: f(x) = Σ α_i * y_i * K(x, x_i) + w0
    decision_scores = np.dot(K, alphas * sv_classes) + w0

    # Predictions: sign of decision function
    predictions = np.sign(decision_scores)

    return predictions, decision_scores
```

### Step 4: Visualizing the Decision Boundary

```python
def plot_decision_boundary(traindata, trainclass, svs, sv_classes, alphas, w0):
    """
    Plot the decision boundary, margins, and support vectors.

    This visualization helps understand:
    - How the SVM separates the classes
    - Which points are support vectors
    - The shape of the nonlinear decision boundary
    """
    plt.figure(figsize=(10, 6))

    # Plot the training data
    # Class 1 in blue, Class 2 in red
    class1_mask = trainclass == 1
    class2_mask = trainclass == 2

    plt.scatter(traindata[class1_mask][:, 0], traindata[class1_mask][:, 1],
                color='blue', label='Class 1', s=100, alpha=0.7)
    plt.scatter(traindata[class2_mask][:, 0], traindata[class2_mask][:, 1],
                color='red', label='Class 2', s=100, alpha=0.7)

    # Highlight support vectors with black circles
    plt.scatter(svs[:, 0], svs[:, 1],
                facecolors='none', edgecolors='black',
                s=300, linewidths=2, label='Support Vectors')

    # Create a fine grid to evaluate the decision function
    xlim = plt.xlim()
    ylim = plt.ylim()
    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 200),
                         np.linspace(ylim[0], ylim[1], 200))

    # Evaluate decision function on every point in the grid
    grid = np.c_[xx.ravel(), yy.ravel()]
    K_grid = polynomial_kernel(grid, svs)
    decision_scores = np.dot(K_grid, alphas * sv_classes) + w0
    decision_scores = decision_scores.reshape(xx.shape)

    # Plot the decision boundary (where decision_score = 0)
    plt.contour(xx, yy, decision_scores, levels=[0],
                linewidths=3, colors='black', linestyles='solid')

    # Plot the margins (where decision_score = ±1)
    plt.contour(xx, yy, decision_scores, levels=[-1, 1],
                linewidths=2, colors='black', linestyles='dashed', alpha=0.5)

    # Fill the decision regions with light colors
    plt.contourf(xx, yy, decision_scores, levels=[-np.inf, 0, np.inf],
                 colors=['lightblue', 'lightcoral'], alpha=0.2)

    plt.title('Nonlinear SVM with Polynomial Kernel', fontsize=14, fontweight='bold')
    plt.xlabel('Feature 1', fontsize=12)
    plt.ylabel('Feature 2', fontsize=12)
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
```

### Step 5: Putting It All Together

```python
# Example usage
if __name__ == "__main__":
    # Load your data (replace with actual data loading)
    # traindata = np.loadtxt('t030.csv', delimiter=',', usecols=(0,1))
    # trainclass = np.loadtxt('t030.csv', delimiter=',', usecols=(2,))

    # For demonstration, create synthetic nonlinearly separable data
    np.random.seed(42)

    # Generate circular data
    n_samples = 100
    radius1 = 2
    radius2 = 4

    # Inner circle (class 1)
    theta1 = np.random.uniform(0, 2*np.pi, n_samples//2)
    r1 = np.random.uniform(0, radius1, n_samples//2)
    X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])
    y1 = np.ones(n_samples//2)

    # Outer ring (class 2)
    theta2 = np.random.uniform(0, 2*np.pi, n_samples//2)
    r2 = np.random.uniform(radius1 + 0.5, radius2, n_samples//2)
    X2 = np.column_stack([r2 * np.cos(theta2), r2 * np.sin(theta2)])
    y2 = 2 * np.ones(n_samples//2)

    traindata = np.vstack([X1, X2])
    trainclass = np.hstack([y1, y2])

    # Train the SVM
    print("Training Nonlinear SVM...")
    svs, sv_classes, alphas, w0 = svm_nonlinear(traindata, trainclass, C=1.0)

    print(f"Number of support vectors: {len(svs)} out of {len(traindata)} training samples")
    print(f"Bias term (w0): {w0:.4f}")

    # Visualize
    plot_decision_boundary(traindata, trainclass, svs, sv_classes, alphas, w0)

    # Make predictions on training data
    predictions, scores = predict_svm(traindata, svs, sv_classes, alphas, w0)
    accuracy = np.mean((predictions == np.where(trainclass == 2, -1, 1)))
    print(f"Training accuracy: {accuracy * 100:.2f}%")
```

---

## Understanding the Output

When you run this code, you'll see:

1. **Support Vectors**: Typically only a small fraction of training points become support vectors. These are the "critical" points that define the boundary.

2. **Decision Boundary**: The black solid line shows where the classifier switches between classes (decision_score = 0).

3. **Margins**: The dashed lines show the margin boundaries (decision_score = ±1). Support vectors lie on or within these margins.

4. **Decision Regions**: The shaded areas show which class the SVM predicts for each region of space.

---

## Hyperparameter Tuning Tips

### Choosing C (Regularization Parameter)

```python
# Try different C values
C_values = [0.01, 0.1, 1.0, 10.0, 100.0]

for C in C_values:
    svs, sv_classes, alphas, w0 = svm_nonlinear(traindata, trainclass, C=C)
    print(f"C={C}: {len(svs)} support vectors")
```

- **Small C** $(0.01-0.1)$: More support vectors, wider margin, better generalization
- **Large C** $(10-100)$: Fewer support vectors, narrower margin, risk of overfitting

### Choosing Polynomial Degree

```python
# Modify the kernel function to accept different degrees
def svm_with_degree(traindata, trainclass, C, degree):
    # Modify polynomial_kernel calls to use specified degree
    K = polynomial_kernel(traindata, traindata, degree=degree)
    # ... rest of the training code
```

- **degree=2**: Good starting point for most problems
- **degree=3-4**: More complex boundaries, risk of overfitting
- **degree>5**: Rarely useful, very high computational cost

---

## Common Pitfalls and Solutions

### 1. Numerical Instability

**Problem**: Kernel values become too large or too small
**Solution**: Feature normalization

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
traindata = scaler.fit_transform(traindata)
```

### 2. All Points Become Support Vectors

**Problem**: C is too large or data has no clear separation
**Solution**: Reduce C or try a different kernel

### 3. Poor Performance

**Problem**: Wrong kernel choice or bad hyperparameters
**Solution**: Use cross-validation to tune C and kernel parameters

### 4. Slow Training

**Problem**: Dataset is too large
**Solution**: Consider using scikit-learn's SVC with better optimization or subsample the data

---

## Extending This Implementation

Want to take this further? Try:

1. **Implement RBF kernel**: Replace polynomial with Gaussian kernel
2. **Add cross-validation**: Use k-fold CV to select optimal C
3. **Multi-class classification**: Extend to handle more than two classes using one-vs-rest or one-vs-one
4. **Feature importance**: Analyze which support vectors contribute most
5. **Compare with sklearn**: Validate your implementation against scikit-learn

---

## Conclusion

Support Vector Machines provide an elegant combination of:

- **Geometric intuition** (maximum margin principle)
- **Mathematical rigor** (convex optimization)
- **Practical power** (kernel trick for nonlinearity)

By implementing SVM from scratch, we gain deep insights into:

- How the optimization problem is formulated
- Why support vectors are special
- How kernels enable nonlinear classification without explicit feature transformation

While libraries like scikit-learn offer highly optimized implementations, understanding the fundamentals helps you make better choices about when and how to apply SVMs in practice.

---

## References and Further Reading

- **Original SVM Papers**: Vapnik & Cortes (1995)
- **Kernel Methods**: Schölkopf & Smola, "Learning with Kernels"
- **Practical Guide**: Hsu, Chang & Lin, "A Practical Guide to Support Vector Classification"
- **Implementation**: <a href="scikit-learn SVM documentation">https://scikit-learn.org/stable/modules/svm.html</a>

---

## Code Repository

The complete implementation with example datasets is available on <a href="GitHub -SVM">https://github.com/OLIVIERKANAMUGIRE/Support-Vector-Machines-from-Scratch</a>

Dataset used: `t030.csv` - A small nonlinearly separable 2D dataset for demonstration.

---

_Happy learning! If you have questions or suggestions, feel free to reach out or open an issue on GitHub._

## References

- Cortes, C., & Vapnik, V. (1995) "Support-vector networks" Machine Learning, 20(3), 273-297. [The original paper introducing Support Vector Machines]

- Schölkopf, B., & Smola, A. J. (2002) "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond" MIT Press, Cambridge, MA.
  [Comprehensive reference on kernel methods and SVMs]

- Hsu, C. W., Chang, C. C., & Lin, C. J. (2003) "A Practical Guide to Support Vector Classification" Technical Report, Department of Computer Science, National Taiwan University. Available: https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf [Essential practical guide for parameter selection and implementation]

- Hastie, T., Tibshirani, R., & Friedman, J. (2009) "The Elements of Statistical Learning: Data Mining, Inference, and Prediction" (2nd ed.) Springer, New York. Chapter 12: Support Vector Machines [Comprehensive statistical perspective on SVMs]
  Burges, C. J. (1998)

- "A Tutorial on Support Vector Machines for Pattern Recognition" Data Mining and Knowledge Discovery, 2(2), 121-167. [Clear and accessible tutorial on SVM fundamentals] Bishop, C. M. (2006)

- "Pattern Recognition and Machine Learning" Springer, New York. Chapter 7: Sparse Kernel Machines [Modern treatment with Bayesian perspective] Cristianini, N., & Shawe-Taylor, J. (2000)
