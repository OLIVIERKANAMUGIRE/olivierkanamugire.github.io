<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> K-Nearest Neighbor (KNN) -- How your closest connections define you! | Olivier KANAMUGIRE </title> <meta name="author" content="Olivier KANAMUGIRE"> <meta name="description" content="KNN algorithm from scratch"> <meta name="keywords" content="mathematics, machine learning, computer vision"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/imageforemoji.png?v=866dc3a4731e297a484a8c9dfd3456fd"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://olivierkanamugire.github.io/blog/2024/knn/"> <script src="/assets/js/theme.js?v=a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Olivier</span> KANAMUGIRE </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Writings </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">K-Nearest Neighbor (KNN) -- How your closest connections define you!</h1> <p class="post-meta"> Created on December 15, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/formatting"> <i class="fa-solid fa-hashtag fa-sm"></i> formatting</a>   <a href="/blog/tag/code"> <i class="fa-solid fa-hashtag fa-sm"></i> code</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>K-Nearest Neighbor (KNN) is an intuitive, supervised machine learning algorithm named a lazy learner. That is because rather than constructing an explicit training model, KNN makes predictions by directly relying on the surrounding data points—its <code class="language-plaintext highlighter-rouge">neighbors</code>—to classify or determine the label of a new instance. This behavior can be analogized to human interactions: to understand someone, we often observe their social circle, as their friends tend to influence or describe who they are. In the context of KNN, the challenge lies in determining the optimal number of neighbors (k) to consider in order to best describe or classify a new point.</p> <p>From a mathematical perspective, KNN determines “closeness” by calculating <code class="language-plaintext highlighter-rouge">distances</code> between data points. Machines operate on numerical input, so distance metrics serve as a foundation for determining which neighbors are closest. Various distance measures exist, such as the Euclidean distance (L2 norm), Manhattan distance (L1 norm), and others. Selecting the appropriate distance metric is critical, as it influences the algorithm’s performance and suitability for specific problems.</p> <p>In this work, we provide a complete implementation of the KNN algorithm from scratch. We define custom functions to compute the necessary distances, identify neighbors, and make predictions, offering a comprehensive understanding of the model’s inner workings.</p> <h2 id="custom-sum-function">custom sum function</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fixed_custom_sum</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># Sum of all elements
</span>        <span class="k">return</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">row</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Column-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="mi">0</span><span class="p">]))]</span>
    <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># Row-wise sum
</span>        <span class="k">return</span> <span class="p">[</span><span class="nf">sum</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">arr</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Invalid axis. Use 0 for columns or 1 for rows.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="sorting">sorting</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

</code></pre></div></div> <h2 id="distance">distance</h2> <p>Now we define euclidean distance we will utilize. \(d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\)</p> <p>This is very nice distance computing because it is implemented as vector computation. Therefore,By avoiding loops and using vectorized operations for distance calculation, we significantly improved the performance of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
    <span class="n">num_train_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">)</span>
    <span class="n">num_test_samples</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">testdata</span><span class="p">)</span>
    <span class="n">num_features</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">traindata</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_test_samples</span><span class="p">):</span>
        <span class="n">test_sample</span> <span class="o">=</span> <span class="n">testdata</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">test_distances</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_train_samples</span><span class="p">):</span>
            <span class="n">train_sample</span> <span class="o">=</span> <span class="n">traindata</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">squared_diff</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">train_sample</span><span class="p">,</span> <span class="n">test_sample</span><span class="p">)]</span>
            <span class="n">test_distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">custom_sum</span><span class="p">(</span><span class="n">squared_diff</span><span class="p">))</span>
        <span class="n">distances</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">test_distances</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">distances</span>
</code></pre></div></div> <h2 id="majority-vote-for-classification">Majority vote for classification</h2> <p>Here, we choose the class that represents the majority of the closest neighbors, because the labels of the nearest neighbors collectively determine the final prediction for the test point.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">majority_vote</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">label_count</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Find the label with the maximum count
</span>    <span class="n">max_count</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">most_common_label</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">label_count</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">max_count</span><span class="p">:</span>
            <span class="n">max_count</span> <span class="o">=</span> <span class="n">count</span>
            <span class="n">most_common_label</span> <span class="o">=</span> <span class="n">label</span>

    <span class="k">return</span> <span class="n">most_common_label</span>
</code></pre></div></div> <h2 id="knn-implementation-and-its-instantiation">KNN implementation and its instantiation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">traindata</span><span class="p">,</span> <span class="n">trainclass</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">traindata</span> <span class="o">=</span> <span class="n">traindata</span>
        <span class="n">self</span><span class="p">.</span><span class="n">trainclass</span> <span class="o">=</span> <span class="n">trainclass</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">testdata</span><span class="p">):</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="nf">euclidean_metric</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">traindata</span><span class="p">,</span> <span class="n">testdata</span><span class="p">)</span>

        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">distances</span><span class="p">:</span>
            <span class="n">k_indices</span> <span class="o">=</span> <span class="nf">custom_argsort</span><span class="p">(</span><span class="n">d</span><span class="p">)[:</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">]</span>
            <span class="n">k_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">trainclass</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
            <span class="n">k_nearest_labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">majority_vote</span><span class="p">(</span><span class="n">k_labels</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">k_nearest_labels</span>
</code></pre></div></div> <p>Contribution</p> <p>No Libraries for Core Algorithm: This project is a pure implementation of KNN, providing a deeper understanding of how the algorithm works under the hood.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/svm/">Nonlinear Support Vector Machine (SVM) with Polynomial Kernel</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/svdappli/">Singular Value Decomposition for Hyperspectral Image Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/logistic_regression/">Logistic Regression for Ultrasonic Flow Meter Health Classification</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/optimizationAlgorithm/">On the optimization algorithms and learning rate</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/graphNN/">Graphs and Graph Neural Nets</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Olivier KANAMUGIRE. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> </body> </html>